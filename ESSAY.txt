1. You will observe that a large portion of the terms in the dictionary are numbers. However, we normally do not use numbers as
query terms to search. Do you think it is a good idea to remove these number entries from the dictionary and the postings lists?
Can you propose methods to normalize these numbers? How many percentage of reduction in disk storage do you observe after removing/normalizing
these numbers?

a. in general, it should generally be better to just ignore the numbers because it does not appear in queries frequently and we need
 to create an additional dictionary entry for every number, which is very costly

b. a possible way to normalize number is to store a sorted string derived from the number taking all the unique digits from the number,
for example store "7233357888" as "23578"

c. If we store every number, the posting file size is 3.1MB, and if we exclude numbers, the posting file size is 2.8MB, which is 10% reduction.

2. What do you think will happen if we remove stop words from the dictionary and postings file? How does it affect the searching phase?
a. since there are a only limited number of stop words with high frequency, by removing them we will get small reduction in dictionary entries,
but much larger reduction in the posting list.

b. By removing the stop words, we can no longer include stop words in our queries

3. The NLTK tokenizer may not correctly tokenize all terms. What do you observe from the resulting terms produced by sent_tokenize()
and word_tokenize()? Can you propose rules to further refine these results?

a. tokenizer wouldn't remove '\n' or other non-word symbols

b. to tackle this case, we have define our own version of tokenizer using regex: tokenizer = RegexpTokenizer(r'\w+')
With this tokenizer, we are able to remove all other symbols other than words and numbers